# Changelog - Lightweight RAG System

## [2.0.0] - 2024-12-19 - Code Simplification

### Simplified
- **Response Extraction**: Consolidated 7 strategies into 2 simple strategies
  - Primary: Extract after "assistant" marker
  - Fallback: Extract after prompt
- **Chunking**: Merged `chunk_document_semantic` and `chunk_document` into single function
- **Context Creation**: Simplified metadata display (removed confidence levels)
- **Fallback Response**: Simplified to use only the best chunk
- **LLM Parameters**: Removed complex Ollama-style parameters, using direct config values
- **Configuration**: Removed unused similarity thresholds and Ollama-style parameters

### Removed
- Complex response extraction strategies (7 â†’ 2)
- Duplicate chunking functions
- Unused similarity thresholds (HIGH/MEDIUM/LOW)
- Ollama-style parameter dictionary
- Complex confidence calculations
- Redundant fallback logic

### Maintained
- Core RAG functionality
- Multiple LLM backends (LocalLLM, MLXLLM, OllamaLLM)
- Semantic chunking with Q&A pair preservation
- Vector store persistence
- Interactive and batch query modes

### Performance
- Reduced code complexity by ~30%
- Faster response extraction
- Cleaner configuration
- More maintainable codebase

## [1.0.0] - 2024-12-19 - Initial Release

### Features
- Lightweight RAG system with PyTorch and scikit-learn
- Multiple LLM backends (PyTorch, MLX, Ollama)
- Semantic document chunking
- Vector store with JSON persistence
- Interactive and batch query modes
- Performance optimizations (FP16, MPS support)
